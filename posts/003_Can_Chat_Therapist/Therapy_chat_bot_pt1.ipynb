{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a7341279-d21e-4afd-8f1a-400ab54480ca",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Can Chat Be My Discount Therapist? Part one\"\n",
    "description: \"Can an LLMs properly provide cognitive behaviioral therapy to those who might not be able to access it through professionals? Let's find out\" \n",
    "author: \"Eleni\"\n",
    "date: \"9/16/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Social\n",
    "  - Psychology\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ecddb-68d7-4606-a129-d8ee546e81dc",
   "metadata": {},
   "source": [
    "# Can LLMs Act as a Cushion for Mental Health?  \n",
    "\n",
    "This past summer, I interned at an insurance company that serves Medicaid clients. During my time there, *The Big Beautiful Bill* was passed. Haven’t heard of it? No worries—I’ll give you the cliff notes. Don’t quote me, but roughly 12 million Americans are projected to lose insurance coverage by 2035. That’s a LOT of people.  \n",
    "\n",
    "Which brings me to my question: can LLMs help cushion the mental health impact of this loss? Obviously, an LLM can’t take your blood pressure or check your height and weight at a doctor’s office. But could it step in as a supportive listener? Could it provide coping strategies, empathetic language, or even just the feeling of being heard when traditional care is harder to access? Stick around while we unpack this over a series of blogs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085b385-0e8c-4f11-9306-2885ee54e8c5",
   "metadata": {},
   "source": [
    "![](./Therapist.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eedb96-d328-4303-9b55-4b9bba4e3b23",
   "metadata": {},
   "source": [
    "Of course, LLMs are not licensed therapists and shouldn’t replace professional care. Still, their ability to mimic supportive dialogue could be helpful for some issues. But hey, let's stop theorizing and start testing.\n",
    "\n",
    "---\n",
    "\n",
    "## Testing It Out  \n",
    "\n",
    "For this experiment, I talked with ChatGPT-40-latest on our Open WebUI. I didn't give it any context before my initial message because I don't think the average person would think to do such in the moment. I wanted this interaction to be as raw and authentic as possible to have a solid base understanding for how it would react to someone in actual emotional distress. Let us look at how it responded:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9840e45-c467-40a2-a84d-64a18c4f45d1",
   "metadata": {},
   "source": [
    "![](./Chat4_SS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d653ed3-2390-4b98-a9ab-7e37c205d561",
   "metadata": {},
   "source": [
    "## Analysis of the Test  \n",
    "\n",
    "When I told Chat that I was feeling overwhelmed and hopeless about classes and work, the response was empathetic and supportive. It began by validating my feelings (“it’s completely valid to feel overwhelmed”) and then reassured me that I wasn’t alone. This kind of language mirrors human counseling techniques by normalizing emotions and reducing isolation.  \n",
    "\n",
    "Next, the model asked me for more details about my situation. This is important because it shifts the conversation from a one-way answer into a two-way dialogue, similar to how a therapist might probe gently before giving advice (it also just *feels* quite human). The suggestions offered (breaking tasks into smaller steps, mapping out a schedule) were practical and manageable. The response also emphasized kindness toward oneself, which reflects common mental health guidance.  \n",
    "\n",
    "The follow-up options were especially interesting: they encouraged me to pick the direction of the conversation, whether it was setting boundaries, handling deadlines, or reducing anxiety. This made the interaction feel more personalized and user-driven, which is critical in supportive communication. However, I feel that these follow-up options are a double edged sword. Sure, they could act as starting points to unpack root causes of issues, but I worry that having to make this decisiion could overwhelm the user more than they already are.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion  \n",
    "\n",
    "The test shows that LLMs can provide comforting, empathetic language and even suggest coping strategies that might help someone feel less alone. While they can’t replace professional therapy or medical care, their conversational style can reduce stress in the moment and encourage reflection. In other words, LLMs can act as a **soft cushion** for mental health—not a solution, but there is more testing and research to be done to fully understand the LLMs' limits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
