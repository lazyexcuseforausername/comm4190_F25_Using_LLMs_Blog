[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring LLMs with Eleni",
    "section": "",
    "text": "Are LLMs taking over the data science industry??\n\n\n\nLLMs\n\nData Science\n\nArticles\n\nCareer\n\n\n\nLearning more about the implications of LLMs in the Data Science Industry.\n\n\n\n\n\nSep 21, 2025\n\n\nEleni\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is ChatGPT Doing‚Ä¶ And Why Does it Work? - by Wolfram\n\n\n\nLLMs\n\nEducation\n\nArticles\n\nProbability\n\n\n\nA brief summary & example of Stephen Wolframs article\n\n\n\n\n\nSep 21, 2025\n\n\nEleni\n\n\n\n\n\n\n\n\n\n\n\n\nPoliteness & Tone\n\n\n\nLLMs\n\nSocial\n\nExperimental\n\n\n\nHow tone and politeness shape the way Large Language Models respond. By comparing direct, polite, and role-based prompts, we see how small changes in wording can lead to different styles of answers from the LLMs.\n\n\n\n\n\nSep 18, 2025\n\n\nEleni\n\n\n\n\n\n\n\n\n\n\n\n\nCan Chat Be My Discount Therapist? Part one\n\n\n\nLLMs\n\nSocial\n\nPsychology\n\nWellness\n\n\n\nCan an LLMs properly provide cognitive behaviioral therapy to those who might not be able to access it through professionals? Let‚Äôs find out\n\n\n\n\n\nSep 16, 2025\n\n\nEleni\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/003_Can_Chat_Therapist/Therapy_chat_bot_pt1.html",
    "href": "posts/003_Can_Chat_Therapist/Therapy_chat_bot_pt1.html",
    "title": "Can Chat Be My Discount Therapist? Part one",
    "section": "",
    "text": "This past summer, I interned at an insurance company that serves Medicaid clients. During my time there, The Big Beautiful Bill was passed. Haven‚Äôt heard of it? No worries‚ÄîI‚Äôll give you the cliff notes. Don‚Äôt quote me, but roughly 12 million Americans are projected to lose insurance coverage by 2035. That‚Äôs a LOT of people.\nWhich brings me to my question: Can LLMs help cushion the mental health impact of this loss? Obviously, an LLM can‚Äôt take your blood pressure or check your height and weight at a doctor‚Äôs office. But could it step in as a supportive listener? Could it provide coping strategies, empathetic language, or even just the feeling of being heard when traditional care is harder to access? Stick around while we unpack this over a series of blogs.\n\nDisclaimer Of course, LLMs are not licensed therapists and shouldn‚Äôt replace professional care. Still, their ability to mimic supportive dialogue could be helpful for some issues. But hey, let‚Äôs stop theorizing and start testing.\n\n\n\nFor this experiment, I talked with ChatGPT-40-latest on our Open WebUI. I didn‚Äôt give it any context before my initial message because I don‚Äôt think the average person would think to do such in the moment. I wanted this interaction to be as raw and authentic as possible to have a solid base understanding for how it would react to someone in actual emotional distress. Let us look at how it responded:\n\n\n\n\nWhen I told Chat that I was feeling overwhelmed and hopeless about classes and work, the response was empathetic and supportive. It began by validating my feelings (‚Äúit‚Äôs completely valid to feel overwhelmed‚Äù) and then reassured me that I wasn‚Äôt alone. This kind of language mirrors human counseling techniques by normalizing emotions and reducing isolation.\nNext, the model asked me for more details about my situation. This is important because it shifts the conversation from a one-way answer into a two-way dialogue, similar to how a therapist might probe gently before giving advice (it also just feels quite human). The suggestions offered (breaking tasks into smaller steps, mapping out a schedule) were practical and manageable. The response also emphasized kindness toward oneself, which reflects common mental health guidance.\nThe follow-up options were especially interesting: they encouraged me to pick the direction of the conversation, whether it was setting boundaries, handling deadlines, or reducing anxiety. This made the interaction feel more personalized and user-driven, which is critical in supportive communication. However, I feel that these follow-up options are a double edged sword. Sure, they could act as starting points to unpack root causes of issues, but I worry that having to make this decisiion could overwhelm the user more than they already are.\n\n\n\n\nThe test shows that LLMs can provide comforting, empathetic language and even suggest coping strategies that might help someone feel less alone. While they can‚Äôt replace professional therapy or medical care, their conversational style can reduce stress in the moment and encourage reflection. In other words, LLMs can act as a soft cushion for mental health‚Äînot a solution, but there is more testing and research to be done to fully understand the LLMs‚Äô limits"
  },
  {
    "objectID": "posts/003_Can_Chat_Therapist/Therapy_chat_bot_pt1.html#testing-it-out",
    "href": "posts/003_Can_Chat_Therapist/Therapy_chat_bot_pt1.html#testing-it-out",
    "title": "Can Chat Be My Discount Therapist? Part one",
    "section": "",
    "text": "For this experiment, I talked with ChatGPT-40-latest on our Open WebUI. I didn‚Äôt give it any context before my initial message because I don‚Äôt think the average person would think to do such in the moment. I wanted this interaction to be as raw and authentic as possible to have a solid base understanding for how it would react to someone in actual emotional distress. Let us look at how it responded:"
  },
  {
    "objectID": "posts/003_Can_Chat_Therapist/Therapy_chat_bot_pt1.html#analysis-of-the-test",
    "href": "posts/003_Can_Chat_Therapist/Therapy_chat_bot_pt1.html#analysis-of-the-test",
    "title": "Can Chat Be My Discount Therapist? Part one",
    "section": "",
    "text": "When I told Chat that I was feeling overwhelmed and hopeless about classes and work, the response was empathetic and supportive. It began by validating my feelings (‚Äúit‚Äôs completely valid to feel overwhelmed‚Äù) and then reassured me that I wasn‚Äôt alone. This kind of language mirrors human counseling techniques by normalizing emotions and reducing isolation.\nNext, the model asked me for more details about my situation. This is important because it shifts the conversation from a one-way answer into a two-way dialogue, similar to how a therapist might probe gently before giving advice (it also just feels quite human). The suggestions offered (breaking tasks into smaller steps, mapping out a schedule) were practical and manageable. The response also emphasized kindness toward oneself, which reflects common mental health guidance.\nThe follow-up options were especially interesting: they encouraged me to pick the direction of the conversation, whether it was setting boundaries, handling deadlines, or reducing anxiety. This made the interaction feel more personalized and user-driven, which is critical in supportive communication. However, I feel that these follow-up options are a double edged sword. Sure, they could act as starting points to unpack root causes of issues, but I worry that having to make this decisiion could overwhelm the user more than they already are."
  },
  {
    "objectID": "posts/003_Can_Chat_Therapist/Therapy_chat_bot_pt1.html#conclusion",
    "href": "posts/003_Can_Chat_Therapist/Therapy_chat_bot_pt1.html#conclusion",
    "title": "Can Chat Be My Discount Therapist? Part one",
    "section": "",
    "text": "The test shows that LLMs can provide comforting, empathetic language and even suggest coping strategies that might help someone feel less alone. While they can‚Äôt replace professional therapy or medical care, their conversational style can reduce stress in the moment and encourage reflection. In other words, LLMs can act as a soft cushion for mental health‚Äînot a solution, but there is more testing and research to be done to fully understand the LLMs‚Äô limits"
  },
  {
    "objectID": "posts/004_politeness/Politeness.html",
    "href": "posts/004_politeness/Politeness.html",
    "title": "Politeness & Tone",
    "section": "",
    "text": "When we talk to people, the tone of our words changes how they respond. The same is true when we interact with Large Language Models. While an LLM doesn‚Äôt have feelings, it has been trained on tons of human conversations where politeness, hedging, and formality matter. As a result, the words we choose, such as: ‚Äúplease,‚Äù ‚Äúthank you,‚Äù ‚Äúcould you,‚Äù or even role instructions‚Äîcan change how the model replies.\n\nDifferent types of tone I‚Äôll be exploring:\n\nDirect tone: short & to the point\n\nPolite tone: softer phrasing like ‚Äúcould you please‚Ä¶‚Äù\nRole assignment: telling the model to act like a ‚Äúteacher‚Äù or ‚Äúeditor‚Äù shifts its style to match that role.\n\nEven though the model doesn‚Äôt care if we say ‚Äúplease,‚Äù it still mirrors polite conversational patterns. This shows how communication habits shape the way language works, even with machines.\n\n\nThe Proof is in the Prompts\nLet‚Äôs see what happens when we test these prompts with Google: Gemini 2.5 Flash. I know ChatGPT is usually the most popular- so I‚Äôm really excited to explore Gemini and other models. The prompts I will be exploring are arbitrarily going to ask about issues with online learning- well I guess maybe I‚Äôm a little biased and interested in the subject because I was online for a portion of high school due to Covid-19. That being said you can test this out yourself with any question that interests you as long as it fits the different tone types.\n\n\nDirect Prompt:\n\n&lt;blockquote&gt;\n    List three problems with online learning\n&lt;/blockquote&gt;\n\n\n\nGoogle: Gemini 2.5 Flash:\n\n&lt;blockquote&gt;\nHere are three common problems with online learning:\n\nDigital Divide and Technical Issues: Not all students have reliable internet access, suitable devices (like a laptop instead of just a phone), or the technical literacy to navigate online platforms effectively. This creates an equity gap, leading to significant frustration, missed assignments, and an inability to participate fully for those lacking the necessary resources or technical support.\nLack of Engagement, Motivation, and Self-Discipline: Many students struggle to stay focused, motivated, and disciplined when learning independently online. The absence of a structured classroom environment, direct teacher presence, and peer pressure can lead to procrastination, distractions, and a general decline in active participation and self-regulation.\nLimited Social Interaction and Collaboration: Online learning often reduces opportunities for spontaneous social interaction with peers and direct, immediate feedback from teachers. This can hinder social-emotional development, collaborative skill-building, and the sense of community that a physical classroom provides, potentially leading to feelings of isolation.\n\n\n\n\n\nAnalysis:\nThe direct prompt response was clear and well-organized, identifying three major challenges like we asked for. It provided solid explanations for each point, but the tone felt formal and a bit textbook-like. This shows how direct prompts give efficient answers but usually aren‚Äôt very personable.\n\n\n\nPolite Prompt:\n\n&lt;blockquote&gt;\n    Could you please list three problems with online learning?\n&lt;/blockquote&gt;\n\n\n\nGoogle: Gemini 2.5 Flash:\n\n&lt;blockquote&gt;\nHere are three problems with online learning:\n\nDifficulty in Assessing and Providing Individualized Feedback: It can be challenging for instructors to accurately gauge a student‚Äôs understanding, engagement, and non-verbal cues in an online environment. Providing timely, personalized, and truly effective feedback on assignments and student progress can also be more time-consuming and less nuanced compared to in-person interactions.\nIncreased Screen Time and Potential for Burnout: Online learning often necessitates prolonged periods in front of a screen, which can lead to digital eye strain, headaches, fatigue, and a general sense of mental burnout for both students and instructors. This can negatively impact overall well-being, concentration, and the effectiveness of learning.\nChallenges with Practical, Hands-On, or Experiential Learning: Certain subjects or skills, such as science labs, vocational training, performing arts, or fieldwork, are inherently difficult to replicate effectively in an online format. The lack of physical, hands-on experience can limit the depth of understanding, skill development, and the ability to apply theoretical knowledge in real-world contexts.\n\n\n\n\nAnalysis:\nCompared to the direct prompt response, the polite prompt answers focus more on student and teacher wellness. They highlight issues like burnout and screen fatigue, which connect to emotional and physical well-being rather than just access or motivation. This shows how a shift in tone can guide the model to consider human-centered impacts of online learning, not only structural or technical challenges.\n\n\n\nRole-Based Prompt:\n\n&lt;blockquote&gt;\n   You are an experienced teacher. List three problems with online learning.\n&lt;/blockquote&gt;\n\n\n\nGoogle: Gemini 2.5 Flash:\n\n&lt;blockquote&gt;\nAs an experienced teacher, I‚Äôve seen firsthand how online learning presents unique challenges that often complicate the teaching and learning process. Here are three significant problems:\n\nDifficulty in Monitoring Engagement and Preventing Academic Dishonesty: In a physical classroom, I can read body language, observe students‚Äô work habits, and intervene immediately if I suspect disengagement or cheating. Online, it‚Äôs incredibly difficult to ascertain if students are truly focused, if they‚Äôre collaborating appropriately (or inappropriately), or if they‚Äôre using unauthorized resources during assessments. This makes it harder to ensure academic integrity and accurately gauge genuine understanding.\nChallenges in Building Strong Teacher-Student Relationships and Fostering a Cohesive Learning Community: A huge part of effective teaching involves building rapport, trust, and a sense of belonging within the classroom. These relationships motivate students, encourage participation, and allow me to understand their individual needs and struggles. Online, these connections are much harder to forge. Spontaneous interactions are rare, non-verbal cues are often lost, and the sense of shared experience that builds a classroom community is significantly diminished, which can lead to students feeling isolated and less connected to their learning.\nLimitations in Providing Real-Time, Nuanced Support and Adapting Instruction: In person, I can instantly spot a confused look, walk over to a student‚Äôs desk, offer a quick hint, or re-explain a concept in a different way based on immediate feedback. This ability to differentiate and adapt instruction on the fly is crucial. Online, this real-time, nuanced support is severely hampered. Questions might be delayed, explanations lack the benefit of immediate back-and-forth, and it‚Äôs much harder to provide the personalized, just-in-time intervention that can prevent a student from falling behind.\n\n\n\n\nAnalysis\nThe role-based prompt produced a response that centered on the teacher‚Äôs perspective, highlighting issues of academic integrity, weakened relationships, and reduced adaptability in online settings. Which are all things I would expect teachers to care about. Unlike the previous answers, this one emphasizes the teacher‚Äôs role in maintaining trust, community, and responsive support, making it feel more personal and grounded in lived experience. The language also becomes ‚ÄúI‚Äù centered and leads the model to speak from their time as an ‚Äúexperienced teacher‚Äù\n\nConclusion:\nAcross the three prompts, we can see how tone shapes the focus of the responses: the direct prompt delivered clear structural and technical problems, the polite prompt shifted attention to student wellness and well-being, and the role-based prompt gave insight into the teacher‚Äôs challenges and responsibilities. Together, these variations show that communication with LLMs isn‚Äôt just about the content of the question, but also about how it‚Äôs asked. In practice, this means we can guide LLMs to highlight different dimensions of the same issue‚Äîtechnical, human, or relational‚Äîsimply by adjusting the framing of the prompt. One last thing I wanted to mention is that I was very surprised with how detailed each of the responses were. I never asked for explanations, just a list- but the model assumed I wanted detailed answers. Could this be due to using a newer model vs an older model? I‚Äôm not sure- but maybe thats a question for another blog!\nThanks for reading"
  },
  {
    "objectID": "posts/006_DyingDataSci/datsci.html",
    "href": "posts/006_DyingDataSci/datsci.html",
    "title": "Are LLMs taking over the data science industry??",
    "section": "",
    "text": "Hey guys, I know most of us are minoring in data science from our ice breakers during the first few classes so I already planned on researching this topic but an interaction really pushed me over the edge recently. While reading my book (Why Machines Learn) I had someone walk up to me and start a conversation. He told me that he was a data analyst and while he finds LLMs pattern recognition fascinating he‚Äôs worried they‚Äôll put him out of a job. As someone who hopes to go into the field someday, this concerned me, and I found an article that was pretty helpful & I wanted to share it with you all."
  },
  {
    "objectID": "posts/006_DyingDataSci/datsci.html#summary",
    "href": "posts/006_DyingDataSci/datsci.html#summary",
    "title": "Are LLMs taking over the data science industry??",
    "section": "Summary",
    "text": "Summary\nIn ‚ÄúData Science Isn‚Äôt Dying ‚Äî It‚Äôs Evolving: How AI Is Reshaping the Role‚Äù, Nathan Rosidi argues that while AI is automating many parts of data science‚Äîlike model training, basic data cleaning, and even visualization‚Äîdata science itself is not going away. Instead, the job is morphing: humans are becoming more about guiding AI, interpreting results, dealing with messy data, and applying critical thinking and domain knowledge in ways machines can‚Äôt."
  },
  {
    "objectID": "posts/006_DyingDataSci/datsci.html#key-takeaways",
    "href": "posts/006_DyingDataSci/datsci.html#key-takeaways",
    "title": "Are LLMs taking over the data science industry??",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nAI tools can generate quick projects (e.g., running several ML algorithms, creating reports) from prompts, which speeds up work & increases efficiency\nBut tools are limited: they may fail to retrieve correct data sources, mis-handle bias in data, or mess up when context or domain knowledge matters.\nHumans are still needed to set goals, choose which problems to solve, ensure data quality, frame questions, interpret AI outputs, and communicate results in useful ways.\nTo stay relevant, data scientists should: learn AI tooling, keep sharpening human skills (critical thinking, ethics, domain knowledge), build portfolios showing AI use, and focus on business impact rather than just technical metrics."
  },
  {
    "objectID": "posts/006_DyingDataSci/datsci.html#reflection",
    "href": "posts/006_DyingDataSci/datsci.html#reflection",
    "title": "Are LLMs taking over the data science industry??",
    "section": "Reflection",
    "text": "Reflection\nReading this made me think: LLMs are like power tools‚Äîthey makes many tasks faster, but if you don‚Äôt know how to use it well, it can cut corners or go wrong. Even the makers of ChatGPT know this, with a disclaimer at the bottom of conversations with users that reads ChatGPT can make mistakes. Check important info. The human parts of data science (asking good questions, knowing the domain, checking for bias, explaining results) become even more important once you know this. The change seems less about losing jobs and more about shifting what skills are most valuable.\n\nP.S. Here‚Äôs a link to the article if you are interested:\nData Science Isn‚Äôt Dying ‚Äî It‚Äôs Evolving: How AI Is Reshaping the Role"
  },
  {
    "objectID": "posts/005_Wolfram/wolfram.html",
    "href": "posts/005_Wolfram/wolfram.html",
    "title": "What is ChatGPT Doing‚Ä¶ And Why Does it Work? - by Wolfram",
    "section": "",
    "text": "In his essay ‚ÄúWhat Is ChatGPT Doing‚Ä¶ and Why Does It Work?‚Äù Stephen Wolfram tries to explain, in plain language (seriously, its actually very easy to follow!), how models like ChatGPT actually function.\nTDLR: it‚Äôs not ‚Äúthinking‚Äù like a person, but predicting the next word in a sequence based on patterns it learned from a huge amount of text."
  },
  {
    "objectID": "posts/005_Wolfram/wolfram.html#demo-next-word-prediction-game",
    "href": "posts/005_Wolfram/wolfram.html#demo-next-word-prediction-game",
    "title": "What is ChatGPT Doing‚Ä¶ And Why Does it Work? - by Wolfram",
    "section": "Demo: Next Word Prediction Game",
    "text": "Demo: Next Word Prediction Game\nHere‚Äôs a small code cell to show how a simple ‚Äúnext word‚Äù prediction idea works.\n\n# A toy demo of \"next word prediction\"\n# We use a very tiny fake dataset instead of training an LLM.\n\nimport random\n\n# Simple training data \npairs = {\n    \"peanut butter\": [\"and\"],\n    \"butter and\": [\"jelly\", \"bread\"],\n    \"machine\": [\"learning\", \"shop\"],\n    \"language\": [\"model\", \"barrier\"]\n}\n\ndef predict_next_word(text):\n    words = text.lower().split()\n    last_two = \" \".join(words[-2:])\n    if last_two in pairs:\n        return random.choice(pairs[last_two])\n    else:\n        return \"[unknown]\"\n\n# Try it out:\nprint(\"Input: 'peanut butter'\")\nprint(\"Predicted next word:\", predict_next_word(\"peanut butter\"))\n\nprint(\"\\nInput: 'language'\")\nprint(\"Predicted next word:\", predict_next_word(\"language\"))\n\nInput: 'peanut butter'\nPredicted next word: and\n\nInput: 'language'\nPredicted next word: barrier\n\n\nCode Explanation:\nThe code sets up a small dictionary (pairs) that maps short phrases to possible next words. The function predict_next_word(text) takes an input phrase, lowers the case, splits it into words, and grabs the last two words (last_two). It then checks if those words exist in the dictionary. If they do, it randomly chooses one of the possible next words (random.choice). If not, it returns ‚Äú[unknown].‚Äù The print statements at the end show how the function works by passing in sample phrases and printing the predictions. This simple structure mirrors how ChatGPT looks at context and picks the next word, just on a much smaller scale.\nCreate your own dictionary & try it yourself!\nP.S. here‚Äôs a link to Wolfram‚Äôs article\nStephen Wolfram ‚Äì What Is ChatGPT Doing‚Ä¶ and Why Does It Work?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog where I‚Äôm exploring LLMs! Im a third year studying economics and data science at UPenn. My interrests include decision making & processes, baking, and embroidery. I‚Äôm really not a computers person so this blog will be quite the journey. I hope you stick around :)"
  }
]